{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## via numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension (number columns in data set);\n",
    "# H is hidden dimension (number neurons in hidden layer); D_out is output dimension (1 for regression)\n",
    "N, D_in, H, D_out = 64, 20, 32, 1\n",
    "\n",
    "# Create random input and output data\n",
    "train_X = np.random.randn(N, D_in)\n",
    "train_y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H) \n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in range(500):\n",
    "# Forward pass: compute predicted y\n",
    "h = train_X.dot(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_relu = np.maximum(h, 0) # apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h_relu.dot(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19485.285725282018\n"
     ]
    }
   ],
   "source": [
    "# Compute and print loss\n",
    "loss = np.square(y_pred - train_y).sum()\n",
    "print(loss) # loss is sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "grad_y_pred = 2.0 * (y_pred - train_y) # derivative of x^2 = 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tIeHLnjs5U8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w2 = h_relu.T.dot(grad_y_pred) \n",
    "# per link, dCost/dw2 = grad_y_pred * 1 {the derivative of relu} * result of previous layer\n",
    "# dot product sums over all observations in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "grad_h = grad_h_relu.copy()\n",
    "grad_h[h < 0] = 0 # recall from forward pass which results were zeroed out\n",
    "grad_w1 = train_X.T.dot(grad_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "w1 -= learning_rate * grad_w1\n",
    "w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19255.19118808303\n",
      "20 472.1624803091429\n",
      "40 239.59100564649518\n",
      "60 148.7971666789395\n",
      "80 103.352680014046\n",
      "100 77.06826467807062\n",
      "120 60.16201383849319\n",
      "140 48.73859486213098\n",
      "160 40.57118170721073\n",
      "180 34.41852545608408\n",
      "200 29.635680546064506\n",
      "220 25.846811205353816\n",
      "240 22.701022269610945\n",
      "260 19.95323527014228\n",
      "280 17.667779751774923\n",
      "300 15.74141370075517\n",
      "320 14.146656939351837\n",
      "340 12.77885546757338\n",
      "360 11.587707732131584\n",
      "380 10.55560708871268\n",
      "400 9.67335896431487\n",
      "420 8.901098924726499\n",
      "440 8.209228124446325\n",
      "460 7.583181674795384\n",
      "480 7.013508421024827\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = train_X.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - train_y).sum()\n",
    "    if t % 20 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = train_X.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy \n",
    "## add hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1 is first hidden layer size\n",
    "# H2 is second hidden layer size (number neurons in hidden layer)\n",
    "# D_out is output dimension (1 for regression)\n",
    "num_rows, num_columns, H1, H2, D_out = 64, 6, 8, 8, 1\n",
    "\n",
    "# Create random input and output data\n",
    "train_X = np.random.randn(num_rows, num_columns)\n",
    "train_y = np.random.randn(num_rows, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(num_columns, H1) \n",
    "w2 = np.random.randn(H1, H2)\n",
    "w3 = np.random.randn(H2, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: compute predicted y\n",
    "h1 = train_X.dot(w1)\n",
    "h1_relu = np.maximum(h1, 0) # apply activation function\n",
    "\n",
    "h2 = h1_relu.dot(w2)\n",
    "h2_relu = np.maximum(h2, 0)\n",
    "\n",
    "y_pred = h2_relu.dot(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47437.70612987318\n"
     ]
    }
   ],
   "source": [
    "# Compute and print loss\n",
    "loss = np.square(y_pred - train_y).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "\n",
    "# gradients on w3\n",
    "grad_w3 = h2_relu.T.dot(grad_y_pred)\n",
    "\n",
    "# gradients on w2\n",
    "grad_h2_relu = grad_y_pred.dot(w3.T)\n",
    "grad_h2 = grad_h2_relu.copy()\n",
    "grad_h2[h2 < 0] = 0\n",
    "grad_w2 = h1_relu.T.dot(grad_h2)\n",
    "\n",
    "# gradients on w1\n",
    "grad_h1_relu = grad_h2.dot(w2.T)\n",
    "grad_h1 = grad_h1_relu.copy()\n",
    "grad_h1[h1 < 0] = 0\n",
    "grad_w1 = train_X.T.dot(grad_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "w1 -= learning_rate * grad_w1\n",
    "w2 -= learning_rate * grad_w2\n",
    "w3 -= learning_rate * grad_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5734.375059058963\n",
      "20 151.65888191004706\n",
      "40 90.82856778087958\n",
      "60 72.84758759896233\n",
      "80 63.626916673785026\n",
      "100 58.27132392860417\n",
      "120 54.663650899448676\n",
      "140 52.01462678429429\n",
      "160 49.82332526724022\n",
      "180 47.85373371973661\n",
      "200 46.12789307722356\n",
      "220 44.7055881761286\n",
      "240 43.68622642673291\n",
      "260 42.865341145861166\n",
      "280 42.20435505875182\n",
      "300 41.64374119992173\n",
      "320 41.16982347765008\n",
      "340 40.78530822934714\n",
      "360 40.444327356761136\n",
      "380 40.10547650172527\n",
      "400 39.79937576889613\n",
      "420 39.5147586950423\n",
      "440 39.25162263915564\n",
      "460 39.00164972635194\n",
      "480 38.75741056580283\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = train_X.dot(w1)\n",
    "    h1_relu = np.maximum(h1, 0) # apply activation function\n",
    "\n",
    "    h2 = h1_relu.dot(w2)\n",
    "    h2_relu = np.maximum(h2, 0)\n",
    "\n",
    "    y_pred = h2_relu.dot(w3)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - train_y).sum()\n",
    "    if t % 20 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "\n",
    "    # gradients on w3\n",
    "    grad_w3 = h2_relu.T.dot(grad_y_pred)\n",
    "\n",
    "    # gradients on w2\n",
    "    grad_h2_relu = grad_y_pred.dot(w3.T)\n",
    "    grad_h2 = grad_h2_relu.copy()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "    grad_w2 = h1_relu.T.dot(grad_h2)\n",
    "\n",
    "    # gradients on w1\n",
    "    grad_h1_relu = grad_h2.dot(w2.T)\n",
    "    grad_h1 = grad_h1_relu.copy()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    grad_w1 = train_X.T.dot(grad_h1)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w3 -= learning_rate * grad_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "## bias, (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1 is first hidden layer size\n",
    "# H2 is second hidden layer size (number neurons in hidden layer)\n",
    "# D_out is output dimension (1 for regression)\n",
    "num_rows, num_columns, H1, H2, D_out = 64, 6, 8, 8, 1\n",
    "\n",
    "# Create random input and output data\n",
    "train_X = np.random.randn(num_rows, num_columns)\n",
    "train_y = np.random.randn(num_rows, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(num_columns, H1)\n",
    "b1 = np.random.randn(H1)\n",
    "w2 = np.random.randn(H1, H2)\n",
    "b2 = np.random.randn(H2)\n",
    "w3 = np.random.randn(H2, D_out)\n",
    "b3 = np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: compute predicted y\n",
    "h1 = train_X.dot(w1) + b1\n",
    "h1_relu = np.maximum(h1, 0)\n",
    "\n",
    "h2 = h1_relu.dot(w2) + b2\n",
    "h2_relu = np.maximum(h2, 0)\n",
    "\n",
    "y_pred = h2_relu.dot(w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "\n",
    "# gradients on w3\n",
    "grad_w3 = h2_relu.T.dot(grad_y_pred)\n",
    "grad_b3 = grad_y_pred.T.dot(np.ones((grad_y_pred.shape[0], D_out)))\n",
    "\n",
    "# gradients on w2\n",
    "grad_h2_relu = grad_y_pred.dot(w3.T)\n",
    "grad_h2 = grad_h2_relu.copy()\n",
    "grad_h2[h2 < 0] = 0\n",
    "grad_w2 = h1_relu.T.dot(grad_h2)\n",
    "grad_b2 = grad_h2.T.dot(np.ones((grad_h2.shape[0], 1)))\n",
    "\n",
    "# gradients on w1\n",
    "grad_h1_relu = grad_h2.dot(w2.T)\n",
    "grad_h1 = grad_h1_relu.copy()\n",
    "grad_h1[h1 < 0] = 0\n",
    "grad_w1 = train_X.T.dot(grad_h1)\n",
    "grad_b1 = grad_h1.T.dot(np.ones((grad_h1.shape[0], 1)))\n",
    "\n",
    "# Update weights\n",
    "w1 -= learning_rate * grad_w1\n",
    "w2 -= learning_rate * grad_w2\n",
    "w3 -= learning_rate * grad_w3\n",
    "b1 -= learning_rate * grad_b1.reshape(grad_b1.shape[0],)\n",
    "b2 -= learning_rate * grad_b2.reshape(grad_b2.shape[0],)\n",
    "b3 -= learning_rate * grad_b3.reshape(grad_b3.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 882.3426905838278\n",
      "20 57.367082511837964\n",
      "40 53.047094969196976\n",
      "60 51.49456781339602\n",
      "80 50.462584685652054\n",
      "100 49.75402166291342\n",
      "120 49.219096746397796\n",
      "140 48.8039078585646\n",
      "160 48.4794870525936\n",
      "180 48.220755635755\n",
      "200 48.00732757023129\n",
      "220 47.82895409507565\n",
      "240 47.67699169312294\n",
      "260 47.54551895863878\n",
      "280 47.43034892702598\n",
      "300 47.3282324445873\n",
      "320 47.23677985073668\n",
      "340 47.15556042700672\n",
      "360 47.08243308076378\n",
      "380 47.0154114212327\n",
      "400 46.953761147429745\n",
      "420 46.89799871193284\n",
      "440 46.84612585075281\n",
      "460 46.79793265280642\n",
      "480 46.75249319906141\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = train_X.dot(w1) + b1\n",
    "    h1_relu = np.maximum(h1, 0)\n",
    "\n",
    "    h2 = h1_relu.dot(w2) + b2\n",
    "    h2_relu = np.maximum(h2, 0)\n",
    "\n",
    "    y_pred = h2_relu.dot(w3) + b3\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - train_y).sum()\n",
    "    if t % 20 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "\n",
    "    # gradients on w3\n",
    "    grad_w3 = h2_relu.T.dot(grad_y_pred)\n",
    "    grad_b3 = grad_y_pred.T.dot(np.ones((grad_y_pred.shape[0], D_out)))\n",
    "\n",
    "    # gradients on w2\n",
    "    grad_h2_relu = grad_y_pred.dot(w3.T)\n",
    "    grad_h2 = grad_h2_relu.copy()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "    grad_w2 = h1_relu.T.dot(grad_h2)\n",
    "    grad_b2 = grad_h2.T.dot(np.ones((grad_h2.shape[0], 1)))\n",
    "\n",
    "    # gradients on w1\n",
    "    grad_h1_relu = grad_h2.dot(w2.T)\n",
    "    grad_h1 = grad_h1_relu.copy()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    grad_w1 = train_X.T.dot(grad_h1)\n",
    "    grad_b1 = grad_h1.T.dot(np.ones((grad_h1.shape[0], 1)))\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w3 -= learning_rate * grad_w3\n",
    "    b1 -= learning_rate * grad_b1.reshape(grad_b1.shape[0],)\n",
    "    b2 -= learning_rate * grad_b2.reshape(grad_b2.shape[0],)\n",
    "    b3 -= learning_rate * grad_b3.reshape(grad_b3.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "## sigmoid activation\n",
    "https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1 is first hidden layer size\n",
    "# H2 is second hidden layer size (number neurons in hidden layer)\n",
    "# D_out is output dimension (1 for regression)\n",
    "num_rows, num_columns, H1, H2, D_out = 64, 6, 8, 8, 1\n",
    "\n",
    "# Create random input and output data\n",
    "train_X = np.random.randn(num_rows, num_columns)\n",
    "train_y = np.random.randn(num_rows, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(num_columns, H1)\n",
    "b1 = np.random.randn(H1)\n",
    "w2 = np.random.randn(H1, H2)\n",
    "b2 = np.random.randn(H2)\n",
    "w3 = np.random.randn(H2, D_out)\n",
    "b3 = np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 649.0950071597233\n",
      "20 256.8272289932953\n",
      "40 159.67962001846217\n",
      "60 129.89481623398328\n",
      "80 116.40652001153944\n",
      "100 107.7124135727588\n",
      "120 101.07759970797775\n",
      "140 95.72474966861347\n",
      "160 91.3354236559858\n",
      "180 87.71753756112318\n",
      "200 84.72839281011784\n",
      "220 82.25412890689996\n",
      "240 80.20207818737434\n",
      "260 78.49645699200121\n",
      "280 77.07523281852238\n",
      "300 75.88762768167479\n",
      "320 74.8920790532314\n",
      "340 74.05456650929935\n",
      "360 73.34723874900314\n",
      "380 72.74728839137535\n",
      "400 72.2360309083358\n",
      "420 71.79815136749068\n",
      "440 71.42108884281276\n",
      "460 71.0945335796994\n",
      "480 70.81001639201845\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = train_X.dot(w1) + b1\n",
    "    h1_sigm = 1 / (1 + np.exp(-1 * h1))\n",
    "    \n",
    "\n",
    "    h2 = h1_sigm.dot(w2) + b2\n",
    "    h2_sigm = 1 / (1 + np.exp(-1 * h1))\n",
    "\n",
    "    y_pred = h2_sigm.dot(w3) + b3\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - train_y).sum()\n",
    "    if t % 20 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - train_y)\n",
    "\n",
    "    # gradients on w3\n",
    "    grad_w3 = h2_sigm.T.dot(grad_y_pred)\n",
    "    grad_b3 = grad_y_pred.T.dot(np.ones((grad_y_pred.shape[0], D_out)))\n",
    "\n",
    "    # gradients on w2\n",
    "    grad_h2_sigm = grad_y_pred.dot(w3.T)\n",
    "    grad_h2 = grad_h2_sigm * h2_sigm * (1 - h2_sigm)\n",
    "    grad_w2 = h1_sigm.T.dot(grad_h2)\n",
    "    grad_b2 = grad_h2.T.dot(np.ones((grad_h2.shape[0], 1)))\n",
    "\n",
    "    # gradients on w1\n",
    "    grad_h1_sigm = grad_h2.dot(w2.T)\n",
    "    grad_h1 = grad_h1_sigm * h1_sigm * (1 - h1_sigm)\n",
    "    grad_w1 = train_X.T.dot(grad_h1)\n",
    "    grad_b1 = grad_h1.T.dot(np.ones((grad_h1.shape[0], 1)))\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w3 -= learning_rate * grad_w3\n",
    "    b1 -= learning_rate * grad_b1.reshape(grad_b1.shape[0],)\n",
    "    b2 -= learning_rate * grad_b2.reshape(grad_b2.shape[0],)\n",
    "    b3 -= learning_rate * grad_b3.reshape(grad_b3.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
